import asyncio
import asyncpg
import logging
from typing import List
from openai import AzureOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Azure OpenAI client setup
openai_client = AzureOpenAI(
    api_key="YOUR_AZURE_OPENAI_KEY",
    api_version="2023-05-15",
    azure_endpoint="https://YOUR-RESOURCE-NAME.openai.azure.com/"
)

# PostgreSQL RAG Retriever
class PostgresRAGRetriever:
    def __init__(self, db_config: dict):
        self.db_config = db_config

    async def _get_connection(self):
        return await asyncpg.connect(**self.db_config)

    async def fetch_similar_chunks(self, table: str, embedding: List[float], client_name: str, section: str = None, top_k: int = 5) -> List[str]:
        async with await self._get_connection() as conn:
            if section:
                query = f"""
                    SELECT content FROM {table}
                    WHERE client_name = $1 AND section = $2
                    ORDER BY embedding <-> $3
                    LIMIT {top_k}
                """
                records = await conn.fetch(query, client_name, section, embedding)
            else:
                query = f"""
                    SELECT content FROM {table}
                    WHERE client_name = $1
                    ORDER BY embedding <-> $2
                    LIMIT {top_k}
                """
                records = await conn.fetch(query, client_name, embedding)
            return [r["content"] for r in records]

    async def fetch_combined_chunks(self, tables: List[str], embedding: List[float], client_name: str, section: str = None, top_k: int = 5) -> List[str]:
        chunks = []
        for table in tables:
            result = await self.fetch_similar_chunks(table, embedding, client_name, section, top_k)
            chunks.extend(result)
        return chunks


# LLM Prompt Builder and Invoker
class SectionRegenerator:
    def __init__(self):
        self.section_table_map = {
            "client_overview": ["clients", "profiles"],
            "next_best_actions": ["actions", "history"],
            "client_contacts": ["contacts"],
            "company_news": ["news"],
            "industry_news": ["industry_news"]
        }
        self.section_with_filter = {"client_contacts", "company_news", "industry_news"}

    def build_prompt(self, section: str, new_prompt: str, previous_content: str, rag_chunks: List[str]) -> str:
        context = "\n\n".join(rag_chunks) if rag_chunks else "[No RAG context found]"
        previous = previous_content if previous_content else "[No previous content]"

        prompt = (
            f"You are regenerating the '{section.replace('_', ' ')}' section for a client.\n\n"
            f"--- CONTEXT FROM DATABASE ---\n{context}\n\n"
            f"--- NEW INSTRUCTIONS ---\n{new_prompt}\n\n"
            f"Make sure all points from the prompt are addressed clearly.\n\n"
            f"--- PREVIOUS CONTENT ---\n{previous}\n\n"
            f"Regenerate the section accordingly."
        )
        return prompt

    def call_azure_llm(self, prompt: str) -> str:
        """
        Calls Azure OpenAI model with the constructed prompt.
        """
        logging.info("Calling Azure OpenAI model...")
        response = openai_client.chat.completions.create(
            model="gpt-35-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()

    async def regenerate_section(
        self,
        db_config: dict,
        section: str,
        embedding: List[float],
        new_prompt: str,
        previous_content: str,
        client_name: str
    ) -> str:
        retriever = PostgresRAGRetriever(db_config)
        tables = self.section_table_map.get(section, [])
        section_filter = section if section in self.section_with_filter else None

        rag_chunks = await retriever.fetch_combined_chunks(tables, embedding, client_name, section_filter)
        prompt = self.build_prompt(section, new_prompt, previous_content, rag_chunks)
        return self.call_azure_llm(prompt)


# --- Example usage ---
if __name__ == "__main__":
    db_config = {
        "user": "your_user",
        "password": "your_pass",
        "database": "your_db",
        "host": "localhost",
        "port": 5432
    }

    section = "company_news"  # Example section to regenerate
    client_name = "Acme Corp"
    new_prompt = "Summarize recent announcements, key business deals, and new product launches."
    previous_content = "No recent news available previously."
    dummy_embedding = [0.1] * 1536  # Replace with actual embedding for the new prompt

    async def run():
        regenerator = SectionRegenerator()
        result = await regenerator.regenerate_section(
            db_config=db_config,
            section=section,
            embedding=dummy_embedding,
            new_prompt=new_prompt,
            previous_content=previous_content,
            client_name=client_name
        )
        print(f"\n--- Regenerated Section: {section} ---\n{result}\n")

    asyncio.run(run())
