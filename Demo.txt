import asyncio
import asyncpg
import logging
from typing import List, Dict
from openai import AzureOpenAI

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Azure OpenAI client
openai_client = AzureOpenAI(
    api_key="YOUR_AZURE_OPENAI_KEY",
    api_version="2023-05-15",
    azure_endpoint="https://YOUR-RESOURCE-NAME.openai.azure.com/"
)


class PostgresRAGRetriever:
    """
    Class for retrieving similar text chunks using pgvector and filtering by client and section.
    """

    def __init__(self, db_config: dict):
        self.db_config = db_config

    async def _get_connection(self):
        return await asyncpg.connect(**self.db_config)

    async def fetch_similar_chunks(
        self,
        table: str,
        embedding: List[float],
        client_name: str,
        section_values: List[str] = None,
        apply_section_filter: bool = False,
        top_k: int = 5
    ) -> List[str]:
        async with await self._get_connection() as conn:
            if apply_section_filter and section_values:
                query = f"""
                    SELECT content FROM {table}
                    WHERE client_name = $1 AND section = ANY($2::text[])
                    ORDER BY embedding <-> $3
                    LIMIT $4
                """
                logging.info(f"Querying {table} with section filter: {section_values}")
                records = await conn.fetch(query, client_name, section_values, embedding, top_k)
            else:
                query = f"""
                    SELECT content FROM {table}
                    WHERE client_name = $1
                    ORDER BY embedding <-> $2
                    LIMIT $3
                """
                logging.info(f"Querying {table} with only client_name")
                records = await conn.fetch(query, client_name, embedding, top_k)
            return [r["content"] for r in records]

    async def fetch_combined_chunks(
        self,
        tables: List[Dict],
        embedding: List[float],
        client_name: str,
        top_k: int = 5
    ) -> List[str]:
        """
        Fetch chunks from all tables relevant to the section.
        """
        all_chunks = []
        for table_info in tables:
            table = table_info["name"]
            apply_section = table_info.get("filter_section", False)
            section_values = table_info.get("section_values", [])
            chunks = await self.fetch_similar_chunks(
                table=table,
                embedding=embedding,
                client_name=client_name,
                section_values=section_values,
                apply_section_filter=apply_section,
                top_k=top_k
            )
            all_chunks.extend(chunks)
        return all_chunks


class SectionRegenerator:
    """
    Main class for regenerating a section using RAG context and Azure OpenAI.
    """

    def __init__(self):
        self.section_table_map = {
            "client_overview": [
                {"name": "clients", "filter_section": True, "section_values": ["client_news", "client_summary"]},
                {"name": "profiles", "filter_section": False}
            ],
            "next_best_actions": [
                {"name": "actions", "filter_section": False},
                {"name": "history", "filter_section": False}
            ],
            "client_contacts": [
                {"name": "contacts", "filter_section": True, "section_values": ["client_contacts"]}
            ],
            "company_news": [
                {"name": "news", "filter_section": True, "section_values": ["company_news"]}
            ],
            "industry_news": [
                {"name": "industry_news", "filter_section": True, "section_values": ["industry_news"]}
            ]
        }

    def build_prompt(self, section: str, new_prompt: str, previous_content: str, rag_chunks: List[str]) -> str:
        """
        Build a prompt including context, new prompt, and prior content.
        """
        context = "\n\n".join(rag_chunks) if rag_chunks else "[No RAG context found]"
        previous = previous_content if previous_content else "[No previous content]"
        return (
            f"You are regenerating the '{section.replace('_', ' ')}' section for a client.\n\n"
            f"--- CONTEXT FROM DATABASE ---\n{context}\n\n"
            f"--- NEW INSTRUCTIONS ---\n{new_prompt}\n\n"
            f"Ensure that all points from the input prompt are included clearly and concisely.\n\n"
            f"--- PREVIOUS GENERATED CONTENT ---\n{previous}\n"
        )

    def call_azure_llm(self, prompt: str) -> str:
        """
        Call Azure OpenAI chat completion model with the generated prompt.
        """
        logging.info("Calling Azure OpenAI model...")
        response = openai_client.chat.completions.create(
            model="gpt-35-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()

    async def regenerate_section(
        self,
        db_config: dict,
        section: str,
        embedding: List[float],
        new_prompt: str,
        previous_content: str,
        client_name: str
    ) -> str:
        """
        Regenerates a section with new instructions and previous content.
        """
        retriever = PostgresRAGRetriever(db_config)
        tables = self.section_table_map.get(section, [])
        rag_chunks = await retriever.fetch_combined_chunks(
            tables=tables,
            embedding=embedding,
            client_name=client_name,
            top_k=5
        )
        full_prompt = self.build_prompt(section, new_prompt, previous_content, rag_chunks)
        return self.call_azure_llm(full_prompt)


# --------------------------
# Example usage (standalone)
# --------------------------
if __name__ == "__main__":
    db_config = {
        "user": "your_user",
        "password": "your_password",
        "database": "your_database",
        "host": "localhost",
        "port": 5432
    }

    section = "client_overview"
    client_name = "Acme Corp"
    new_prompt = "Provide a summary of the client's key business areas and strategic direction."
    previous_content = "Acme Corp has been focusing on cloud migration for logistics."
    dummy_embedding = [0.01] * 1536  # Replace with actual embedding vector

    async def main():
        generator = SectionRegenerator()
        result = await generator.regenerate_section(
            db_config=db_config,
            section=section,
            embedding=dummy_embedding,
            new_prompt=new_prompt,
            previous_content=previous_content,
            client_name=client_name
        )
        print(f"\n--- Regenerated '{section}' ---\n{result}")

    asyncio.run(main())
