from typing import List, Dict
from openai import AzureOpenAI  # or openai if using OpenAI directly

class ContextUpdatePlanner:
    def __init__(self, llm_client: AzureOpenAI, pg_conn):
        """
        llm_client: AzureOpenAI client for LLM calls
        pg_conn: PostgreSQL connection for fetching call report/news/website data
        """
        self.llm_client = llm_client
        self.pg_conn = pg_conn

    def _ask_source_type(self, user_prompt: str) -> str:
        """
        Step 1: Ask LLM where to get the new context from.
        Only returns one of: 'call_report', 'news', 'website_news'
        """
        system_prompt = """
        You are a classification assistant.
        Based on the user's request, decide where the new context should be retrieved from.
        Only respond with exactly one of the following values:
        - call_report
        - news
        - website_news

        Rules:
        - If the user mentions "call report", "call notes", "meeting notes" → call_report
        - If the user mentions "news" or "market news" → news
        - If the user mentions "website" or "company website" → website_news
        """
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": user_prompt.strip()}
            ],
            temperature=0
        )
        return response.choices[0].message.content.strip()

    def _fetch_call_report_context(self) -> Dict:
        """
        Fetch call report context, citation IDs, and titles from PostgreSQL.
        Also include previous call report context.
        """
        with self.pg_conn.cursor() as cur:
            cur.execute("SELECT report_id, title, content FROM call_reports ORDER BY created_at DESC LIMIT 5;")
            rows = cur.fetchall()
        context = [{"id": r[0], "title": r[1], "content": r[2]} for r in rows]
        previous_context = " ".join([c["content"] for c in context])
        return {
            "previous_context": previous_context,
            "call_reports": context
        }

    def _fetch_news_context(self) -> Dict:
        """
        Fetch latest news content and citations from your existing news generation logic.
        """
        # This is a placeholder — replace with your actual news generation function
        news_content = "Generated fresh news content..."
        news_citations = [{"id": 101, "url": "https://example.com/news1"}]
        return {
            "news_content": news_content,
            "news_citations": news_citations
        }

    def _fetch_website_news_context(self) -> Dict:
        """
        Fetch website news content (from database or API).
        """
        with self.pg_conn.cursor() as cur:
            cur.execute("SELECT content FROM website_news ORDER BY created_at DESC LIMIT 1;")
            row = cur.fetchone()
        return {
            "website_news_content": row[0] if row else ""
        }

    def _decide_section_updates(self, user_prompt: str, context_data: Dict) -> Dict:
        """
        Step 2: Ask LLM which sections to update, 
        whether to include existing content or new content only, 
        and handle citation mapping.
        """
        system_prompt = """
        You are an update planner.
        You will receive:
        - User's request
        - Retrieved context (call report/news/website news)
        Your job:
        - Decide which sections in the document should be updated
        - For each section, decide if we should include existing content or replace with new content
        - Suggest citation mapping for any new information
        Return as JSON with:
        {
            "sections_to_update": [
                {"name": "<section_name>", "mode": "append|replace", "citations": [<ids>]}
            ]
        }
        """
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": f"User request: {user_prompt}\n\nContext:\n{context_data}"}
            ],
            temperature=0
        )
        return response.choices[0].message.content.strip()

    def process_request(self, user_prompt: str) -> Dict:
        """
        Orchestrates the entire flow:
        1. Detect source type
        2. Fetch context accordingly
        3. Pass to LLM for section update planning
        """
        source_type = self._ask_source_type(user_prompt)

        if source_type == "call_report":
            context_data = self._fetch_call_report_context()
        elif source_type == "news":
            context_data = self._fetch_news_context()
        elif source_type == "website_news":
            context_data = self._fetch_website_news_context()
        else:
            raise ValueError(f"Unexpected source type: {source_type}")

        section_plan = self._decide_section_updates(user_prompt, context_data)

        return {
            "source_type": source_type,
            "context_data": context_data,
            "section_plan": section_plan
        }
